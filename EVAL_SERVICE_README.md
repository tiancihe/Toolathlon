# Toolathlon Remote Evaluation Service

Besides configuring Toolathlon evaluation on your own machine, we also provide Toolathlon evaluation as a service on public servers, where we have setup all the required MCP accounts and you don't need to worry about the setup -- you don't even need to install any MCP-related dependencies, evaluation can be ran by just communicating with our public server.

> We have set up a public Toolathlon evaluation service on 47.253.6.47, this is mainly for you to quickly play with our evaluation without any setup. However, due to the potential evaluation conflict from multiple users, we have constrained this public service to be 3 evaluation requests per IP per 24 hours. If you find this public service crowded, you have a few other options to do the evaluation:
> 1. Setup your own Toolathlon evaluation service on your own machine following the main readme, which would take like 20-30 minutes.
> 2. If you are a major user that will use Toolathlon evaluation a lot, please contact us (jlini@cse.ust.hk / junxianh@cse.ust.hk), we may be able to provide a dedicated evaluation service for you (for free).
> 3. If you have an API endpoint and just want to test your model, please contact us (jlini@cse.ust.hk / junxianh@cse.ust.hk) and we are happy to help you run evaluation on Toolathlon with your given API endpoint.

---

## Quick Start

### Installation

If you want to test **your inhouse locally deployed model that is not publicly accessible**, just simply put `eval_client.py` and `simple_client_ws.py` together under a folder on your own machine (they are already there if you cloned this repo), then install the client-side dependencies:

```bash
pip install httpx typer websockets
# Or use uv
uv add httpx typer websockets
```

### Private Mode (Local OpenAI-ChatCompletion endpoint via vLLM/SGLang etc)

For locally deployed models that are not publicly accessible:

```bash
python eval_client.py run \
  --mode private \
  --base-url http://localhost:8000/v1 \
  --model-name your-model-name \
  --output-dir ./results \
  --server-host 47.253.6.47 \
  --api-key dummy \
  --workers 10 \
  --server-port 8080 \
  --ws-proxy-port 8081
```


### Public Mode (Pubic OpenAI-ChatCompletion endpoint from OpenAI/Anthropic/etc.)

For ready-to-use public API endpoints:

```bash
python eval_client.py run \
  --mode public \
  --base-url https://api.openai.com/v1 \
  --model-name gpt-5 \
  --output-dir ./results \
  --server-host 47.253.6.47 \
  --api-key sk-your-api-key \
  --workers 10 \
  --server-port 8080
```

If the server is idle, your task will be submitted and you will find the results later in the `./results` directory. Otherwise, please wait for a while and check again later via:

```bash
python eval_client.py check 47.253.6.47 --server-port 8080
```

for more details, please use the following commands:
```
python eval_client.py run --help
python eval_client.py check --help
python eval_client.py cancel --help
python eval_client.py status --help
```

We strongly suggest you first try `Test Subset of Tasks` in "Advanced Features" section to run a small subset of tasks to ensure the evaluation is working correctly.

---


## Advanced Features

### Custom Model Parameters (Optional)

Override default model parameters except "model","messages","tools","tool_choice" and "stream" (see `utils/api_model/model_provider.py` for more details), which means the final completion params = these 5 automatically generated by agnt scaffold + your provided params:

```bash
# Create params.json
cat > model_params.json << EOF
{
  "temperature": 0.7,
  "top_p": 0.95,
  "max_tokens": 4096
}
EOF

# Use with eval
python eval_client.py run \
  --mode public \
  --base-url https://api.openai.com/v1 \
  --model-name gpt-4 \
  --output-dir ./results \
  --server-host 47.253.6.47 \
  --api-key sk-... \
  --model-params-file ./model_params.json
```

### Test Subset of Tasks (Optional)

Run evaluation on specific tasks only:

```bash
# Create task list
cat > my_tasks.txt << EOF
ab-testing
find-alita-paper
git-milestone
EOF

# Use with eval
python eval_client.py run \
  --mode public \
  --base-url https://api.openai.com/v1 \
  --model-name gpt-4 \
  --output-dir ./results \
  --server-host 47.253.6.47 \
  --api-key sk-... \
  --task-list-file ./my_tasks.txt
```

**Note:** Each line in the task list file should be a task name. Empty lines are ignored.

### Skip Container Restart (Debugging Only, Optional)

âš ï¸ **For debugging/testing small subsets only. NOT recommended for complete evaluation.**

```bash
python eval_client.py run \
  --mode public \
  --base-url https://api.openai.com/v1 \
  --model-name gpt-4 \
  --output-dir ./results \
  --server-host 47.253.6.47 \
  --api-key sk-... \
  --task-list-file ./debug_tasks.txt \
  --skip-container-restart
```

**When to use:**
- âœ“ Debugging specific issues
- âœ“ Testing 1-2 tasks quickly
- âœ— Complete evaluation (always restart containers for clean environment)

### Override Output Directory (Optional)

By default, the client will error if the output directory is not empty. Use `--override-output-dir` to automatically clear it:

```bash
python eval_client.py run \
  --mode public \
  --base-url https://api.openai.com/v1 \
  --model-name gpt-4 \
  --output-dir ./results \
  --server-host 47.253.6.47 \
  --api-key sk-... \
  --override-output-dir
```

**Behavior:**
- Without flag: Error if directory is not empty
- With flag: Automatically clear and recreate directory

### Resume Incomplete Tasks (Optional)

If your evaluation was interrupted (e.g., network issues, client/server crash), you can resume it by providing the same job ID:

```bash
python eval_client.py run \
  --mode public \
  --base-url https://api.openai.com/v1 \
  --model-name gpt-4 \
  --output-dir ./results \
  --server-host 47.253.6.47 \
  --api-key sk-... \
  --job-id job_abc123def456  # Use the same job ID to resume
```

**Note:**
- You can find your job ID from the initial submission output or from your client.log
- The server will output a warning if the job ID already exists, but will accept the request
- Results will be written to the same output directory on server
- This works for both public and private modes

---

## Monitoring & Management

### Check Server Status

```bash
python eval_client.py check 47.253.6.47 --server-port 8080
```

**Output (idle):**
```
âœ“ Server is idle and ready to accept tasks
```

**Output (busy):**
```
â³ Server is currently busy
   Job ID: job_ab*****56
   Mode: public
   Started: 2025-12-04T10:30:45.123456

Please try again later.
```

### Monitor Progress

```bash
# Watch client log (real-time)
tail -f ./results/client.log

# Watch server execution log (synced in real-time)
tail -f ./results/server.log
```

### Check Task Status

```bash
python eval_client.py status job_abc123def456 47.253.6.47 --server-port 8080
```

**Possible statuses:**
- `running` - Task is currently executing
- `completed` - Task finished successfully
- `failed` - Task failed with error
- `timeout` - Task exceeded time limit (240 minutes)
- `cancelled` - Task was manually cancelled

### Cancel Running Task

```bash
python eval_client.py cancel job_abc123def456 47.253.6.47 --server-port 8080
```

**This will:**
- Kill the evaluation process
- Stop and remove all Docker containers
- Clean up server resources

**Note:** If you try to cancel a completed task, you'll get a friendly message:
```
â„¹ï¸  Job has already completed. Cannot cancel a completed job.

ðŸ’¡ Tip: Use 'status' command to check job status:
   python eval_client.py status job_abc123def456 47.253.6.47 --server-port 8080
```

---

## Output Files

When a task completes, the output directory will contain:

```
./results/
â”œâ”€â”€ client.log              - Client execution log (monitor with 'tail -f')
â”œâ”€â”€ server.log              - Server execution log (synced from server)
â”œâ”€â”€ eval_stats.json         - Final evaluation statistics
â”œâ”€â”€ traj_log_all.jsonl      - Trajectory logs for all tasks
â”œâ”€â”€ finalpool/              - Individual task results (downloaded incrementally)
â”‚   â”œâ”€â”€ task_name_1/
â”‚   â”œâ”€â”€ task_name_2/
â”‚   â””â”€â”€ ...
â””â”€â”€ ws_client.log           - WebSocket client log (private mode only)
```

**File descriptions:**

1. **eval_stats.json**
   - Evaluation statistics and results
   - Contains pass/fail status for all tasks

2. **traj_log_all.jsonl**
   - Complete trajectory logs for all tasks
   - One JSON object per line, each representing one task

3. **client.log**
   - Client-side execution log with timestamps
   - Shows task submission, status polling, completion

4. **server.log**
   - Server-side execution log (synced in real-time)
   - Shows container deployment, parallel test execution

5. **finalpool/**
   - Individual task results and artifacts
   - Downloaded incrementally as tasks complete
   - Each subdirectory contains complete task output

6. **ws_client.log** (private mode only)
   - WebSocket client log
   - Shows WebSocket connection status and request handling

---

## Architecture

### Public Mode
```
Client â†’ Server â†’ OpenAI/Anthropic/etc. API
```
Client submits task with API credentials. Server runs evaluation using the public API.

### Private Mode
```
Client + Local LLM â†â†’ WebSocket â†â†’ Server
```
Server runs evaluation but forwards LLM requests back to client via WebSocket. Your LLM credentials never leave your machine.

**How Private Mode Works:**
1. Server starts `simple_server_ws.py` (WebSocket proxy on port 8081)
2. Client starts `simple_client_ws.py` (connects to proxy)
3. When server needs LLM inference, request flows: Server â†’ WebSocket â†’ Client â†’ Your LLM
4. Response flows back: Your LLM â†’ Client â†’ WebSocket â†’ Server

---

## Privacy & Security

### Public Mode
- âš ï¸ API keys are sent to server for making requests
- âš ï¸ Server does not permanently store API keys
- ðŸ’¡ Use HTTPS in production to protect credentials
- Server uses your API key to call LLM directly

### Private Mode
- âœ“ LLM URL and API key stay on client machine
- âœ“ Server never sees your credentials
- âœ“ Only inference requests/responses transmitted
- Server's `TOOLATHLON_OPENAI_BASE_URL` points to local WebSocket proxy

### Job ID Anonymization
When checking server status, job IDs are anonymized:
- `job_abc123def456` â†’ `job_ab*****56`
- First 6 and last 2 characters shown
- Protects running task privacy

### Rate Limiting
- Public server: 3 tasks per IP per 24 hours (configurable by server admin)
- Prevents abuse and ensures fair access
- Contact us for dedicated access if needed

---

## Get Help

For detailed help on any command:

```bash
# Main help
python eval_client.py --help

# Command-specific help
python eval_client.py run --help
python eval_client.py status --help
python eval_client.py cancel --help
python eval_client.py check --help
```

If you encounter any issues, please contact us:
- Email: jlini@cse.ust.hk / junxianh@cse.ust.hk
- We can help test your model with provided API endpoint
- We can set up dedicated evaluation channels for major users

---

## Server Setup (For Server Administrators)

This section is mainly for developers running the server side. Skip this if you just want to use the service as a client.

### Prerequisites

Server requires full Toolathlon environment:

```bash
# Install dependencies
bash global_preparation/install_env_minimal.sh true

# Deploy local services (Canvas, email, etc.)
bash global_preparation/deploy_containers.sh true

# Install server dependencies
pip install fastapi uvicorn websockets
# Or use uv
uv add fastapi uvicorn websockets
```

### Start Server

```bash
python eval_server.py <server_port> <ws_proxy_port> <max_submissions_per_ip>
```

**Parameters:**
- `server_port` - Main server port (default: 8080)
- `ws_proxy_port` - WebSocket proxy port for private mode (default: 8081)
- `max_submissions_per_ip` - Rate limit per IP per 24h (default: 3, use -1 for unlimited)

**Examples:**

```bash
# Default: 3 submissions per IP per 24h
python eval_server.py 8080 8081

# Unlimited submissions (no rate limiting)
python eval_server.py 8080 8081 -1

# Custom limit: 10 submissions per IP per 24h
python eval_server.py 8080 8081 10
```

**Server output:**

Default (with rate limiting):
```
============================================================
Toolathlon Remote Evaluation Server
============================================================
Server Port: 8080
WebSocket Proxy Port: 8081 (for private mode)
Max tasks per IP: 3 per 24 hours
Timeout: 240 minutes
Output directory: ./dumps_public_service
============================================================
âœ“ WebSocket proxy started (PID: 12345)
  Log: ./dumps_public_service/ws_proxy.log
============================================================
```

Unlimited mode:
```
============================================================
Toolathlon Remote Evaluation Server
============================================================
Server Port: 8080
WebSocket Proxy Port: 8081 (for private mode)
Max tasks per IP: Unlimited (no rate limiting)
Timeout: 240 minutes
Output directory: ./dumps_public_service
============================================================
âœ“ WebSocket proxy started (PID: 12345)
  Log: ./dumps_public_service/ws_proxy.log
============================================================
```

### Server Configuration

- **Timeout:** 240 minutes (4 hours) per task
- **Concurrent tasks:** 1 task at a time (single-job queue)
- **Rate limiting:** Configurable per IP (default 3 per 24h, -1 for unlimited)
- **Output directory:** `./dumps_public_service/` (configurable in code)

### Server Management

**Graceful shutdown:**
Press `Ctrl+C` to trigger graceful shutdown. The server will:
- Stop accepting new tasks
- Kill running evaluation processes
- Clean up Docker containers
- Close WebSocket proxy

**View server logs:**
```bash
# Main server log (stdout)
# Shown in terminal where server is running

# WebSocket proxy log
tail -f ./dumps_public_service/ws_proxy.log

# Individual job logs
tail -f ./dumps_public_service/<job_id>/server_stdout.log
```
